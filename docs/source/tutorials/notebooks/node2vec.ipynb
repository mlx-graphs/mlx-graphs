{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2vec\n",
    "\n",
    "> [!WARNING]\n",
    "> This tutorial is experimental and requires `mlx_cluster` to be installed which\n",
    "> currently requires mlx 0.18.\n",
    "\n",
    "**Goal:** This tutorial will guide you through implementing node2vec to generate vector embeddings for nodes in a simple undirected graph.\n",
    "\n",
    "**Concepts:** `MLX`, `Node2vec`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import mlx.core as mx\n",
    "from mlx_graphs.datasets import PlanetoidDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this first tutorial, we will use the [PlanetoidDataset](https://chrsmrrs.github.io/datasets/docs/datasets/) collection, which comprises of citation networks for `Cora`, `Pubmed` and `CiteSeer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `Cora` dataset consisting of **2708** nodes and **10,056** edges. The dataset can be easily accessed via `PlanetoidDataset` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora data ... Done\n"
     ]
    }
   ],
   "source": [
    "dataset = PlanetoidDataset(\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cora(num_graphs=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access dataset properties directly from `dataset`object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset attributes\n",
      "--------------------\n",
      "Number of graphs: 1\n",
      "Number of node features: 1433\n",
      "Number of edge features: 0\n",
      "Number of graph features: 0\n",
      "Number of graph classes to predict: 0\n",
      "\n",
      "Dataset stats\n",
      "--------------------\n",
      "Mean node degree: 3.90\n",
      "Mean num of nodes: 2708.00\n",
      "Mean num of edges: 10556.00\n"
     ]
    }
   ],
   "source": [
    "# Some useful properties\n",
    "print(\"Dataset attributes\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")\n",
    "print(f\"Number of graph features: {dataset.num_graph_features}\")\n",
    "print(f\"Number of graph classes to predict: {dataset.num_graph_classes}\\n\")\n",
    "\n",
    "# Statistics of the dataset\n",
    "stats = defaultdict(list)\n",
    "for g in dataset:\n",
    "    stats[\"Mean node degree\"].append(g.num_edges / g.num_nodes)\n",
    "    stats[\"Mean num of nodes\"].append(g.num_nodes)\n",
    "    stats[\"Mean num of edges\"].append(g.num_edges)\n",
    "\n",
    "print(\"Dataset stats\")\n",
    "print(\"-\" * 20)\n",
    "for k, v in stats.items():\n",
    "    mean = mx.mean(mx.array(v)).item()\n",
    "    print(f\"{k}: {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(\n",
       "\tedge_index(shape=(2, 10556), int32)\n",
       "\tnode_features(shape=(2708, 1433), float32)\n",
       "\tnode_labels(shape=(2708,), int32)\n",
       "\ttrain_mask(shape=(2708,), bool)\n",
       "\tval_mask(shape=(2708,), bool)\n",
       "\ttest_mask(shape=(2708,), bool))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple neural network using node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_graphs.algorithms import Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify hyperparameters for node2vec. \n",
    "\n",
    "The most important hyperparameters for node2vec are `p` and `q` where \n",
    "1. `p` : specifies the likelihood of revisiting a node in the walk (return parameter). When this is low the algorithm is  more likely to take a step back.\n",
    "2. `q` : specifies likelhood of exploring nodes that are further away from the source. When this is high the algorithm is more likely to explore the neighbourhood\n",
    "3. `embedding_dim`: dimemnsions of embedding model\n",
    "4. `walk_length`: Number of nodes to consider in a walk\n",
    "5. `context_size`: The actual context size which is considered for positive samples. This parameter increases the effective sampling rate by reusing samples across different source nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "walk_length = 50\n",
    "context_size = 10\n",
    "walks_per_node = 10\n",
    "num_negative_samples = 1\n",
    "p = 1.0\n",
    "q = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and train a simple model loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import time\n",
    "optimizer = optim.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, num_epochs=10, batch_size=32):\n",
    "    \"\"\"Your training function\"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        dataloader = model.dataloader(batch_size=batch_size)\n",
    "        for pos, neg in dataloader:\n",
    "            loss, grad = nn.value_and_grad(model, model.loss)(pos, neg)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.update(model, grad)\n",
    "        print(f\"Epoch {epoch}: batch loss = {total_loss/batch_size:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_training(model_creation_fn, optimizer_creation_fn, device='gpu', num_epochs=10, batch_size=32):\n",
    "    \"\"\"\n",
    "    Benchmark training on specified device\n",
    "    \n",
    "    Args:\n",
    "        model_creation_fn: Function that returns a new model instance\n",
    "        optimizer_creation_fn: Function that takes model and returns optimizer\n",
    "        device: 'gpu' or 'cpu'\n",
    "    \"\"\"\n",
    "    print(f\"ðŸš€ {device.upper()} Benchmark\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Set device\n",
    "    if device == 'cpu':\n",
    "        use_gpu = False\n",
    "    else:\n",
    "        use_gpu = True\n",
    "    \n",
    "    # Create fresh model and optimizer for this benchmark\n",
    "    model = model_creation_fn(use_gpu=use_gpu)\n",
    "    optimizer = optimizer_creation_fn(model)\n",
    "    \n",
    "    # Benchmark the training\n",
    "    start_time = time.perf_counter()\n",
    "    training_loop(model, optimizer, num_epochs, batch_size)\n",
    "    del model\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    \n",
    "    print(f\"ðŸŽ¯ {device.upper()} Total Time: {total_time:.3f} seconds\")\n",
    "    print(f\"ðŸ“ˆ Average per epoch: {total_time/num_epochs:.3f} seconds\")\n",
    "    \n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(use_gpu=False):\n",
    "    \"\"\"Create Node2Vec model\"\"\"\n",
    "    model = Node2Vec(\n",
    "        edge_index=dataset[0].edge_index,\n",
    "        num_nodes=dataset[0].num_nodes,\n",
    "        embedding_dim=embedding_dim,\n",
    "        walk_length=walk_length,\n",
    "        context_size=context_size,\n",
    "        walks_per_node=walks_per_node,\n",
    "        num_negative_samples=num_negative_samples,\n",
    "        p=p,\n",
    "        q=q,\n",
    "        use_gpu=use_gpu,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    \"\"\"Create optimizer - replace with your actual optimizer\"\"\"\n",
    "    # Example - update with your actual optimizer\n",
    "    optimizer = optim.Adam(learning_rate=0.001)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmarks...\n",
      "\n",
      "ðŸš€ GPU Benchmark\n",
      "----------------------------------------\n",
      "Epoch 0: batch loss = 2.68540\n",
      "Epoch 1: batch loss = 2.25251\n",
      "Epoch 2: batch loss = 2.22112\n",
      "Epoch 3: batch loss = 2.20850\n",
      "Epoch 4: batch loss = 2.20118\n",
      "Epoch 5: batch loss = 2.19510\n",
      "Epoch 6: batch loss = 2.19153\n",
      "Epoch 7: batch loss = 2.19094\n",
      "Epoch 8: batch loss = 2.18868\n",
      "Epoch 9: batch loss = 2.18604\n",
      "Epoch 10: batch loss = 2.18575\n",
      "Epoch 11: batch loss = 2.18490\n",
      "Epoch 12: batch loss = 2.18312\n",
      "Epoch 13: batch loss = 2.18215\n",
      "Epoch 14: batch loss = 2.18156\n",
      "Epoch 15: batch loss = 2.17968\n",
      "Epoch 16: batch loss = 2.17803\n",
      "Epoch 17: batch loss = 2.17907\n",
      "Epoch 18: batch loss = 2.17829\n",
      "Epoch 19: batch loss = 2.17767\n",
      "Epoch 20: batch loss = 2.17855\n",
      "Epoch 21: batch loss = 2.17641\n",
      "Epoch 22: batch loss = 2.17812\n",
      "Epoch 23: batch loss = 2.17572\n",
      "Epoch 24: batch loss = 2.17765\n",
      "Epoch 25: batch loss = 2.17583\n",
      "Epoch 26: batch loss = 2.17600\n",
      "Epoch 27: batch loss = 2.17441\n",
      "Epoch 28: batch loss = 2.17587\n",
      "Epoch 29: batch loss = 2.17451\n",
      "Epoch 30: batch loss = 2.17305\n",
      "Epoch 31: batch loss = 2.17428\n",
      "Epoch 32: batch loss = 2.17267\n",
      "Epoch 33: batch loss = 2.17418\n",
      "Epoch 34: batch loss = 2.17377\n",
      "Epoch 35: batch loss = 2.17395\n",
      "Epoch 36: batch loss = 2.17216\n",
      "Epoch 37: batch loss = 2.17272\n",
      "Epoch 38: batch loss = 2.17375\n",
      "Epoch 39: batch loss = 2.17363\n",
      "Epoch 40: batch loss = 2.17194\n",
      "Epoch 41: batch loss = 2.17394\n",
      "Epoch 42: batch loss = 2.17159\n",
      "Epoch 43: batch loss = 2.17373\n",
      "Epoch 44: batch loss = 2.17292\n",
      "Epoch 45: batch loss = 2.17181\n",
      "Epoch 46: batch loss = 2.17251\n",
      "Epoch 47: batch loss = 2.17076\n",
      "Epoch 48: batch loss = 2.17147\n",
      "Epoch 49: batch loss = 2.17017\n",
      "Epoch 50: batch loss = 2.17189\n",
      "Epoch 51: batch loss = 2.17062\n",
      "Epoch 52: batch loss = 2.17016\n",
      "Epoch 53: batch loss = 2.17046\n",
      "Epoch 54: batch loss = 2.17159\n",
      "Epoch 55: batch loss = 2.17097\n",
      "Epoch 56: batch loss = 2.17147\n",
      "Epoch 57: batch loss = 2.17205\n",
      "Epoch 58: batch loss = 2.17121\n",
      "Epoch 59: batch loss = 2.17066\n",
      "Epoch 60: batch loss = 2.16996\n",
      "Epoch 61: batch loss = 2.17064\n",
      "Epoch 62: batch loss = 2.17082\n",
      "Epoch 63: batch loss = 2.17007\n",
      "Epoch 64: batch loss = 2.17101\n",
      "Epoch 65: batch loss = 2.17136\n",
      "Epoch 66: batch loss = 2.17181\n",
      "Epoch 67: batch loss = 2.17169\n",
      "Epoch 68: batch loss = 2.16981\n",
      "Epoch 69: batch loss = 2.16975\n",
      "Epoch 70: batch loss = 2.17057\n",
      "Epoch 71: batch loss = 2.16976\n",
      "Epoch 72: batch loss = 2.17201\n",
      "Epoch 73: batch loss = 2.17018\n",
      "Epoch 74: batch loss = 2.16898\n",
      "Epoch 75: batch loss = 2.16988\n",
      "Epoch 76: batch loss = 2.17091\n",
      "Epoch 77: batch loss = 2.16826\n",
      "Epoch 78: batch loss = 2.16960\n",
      "Epoch 79: batch loss = 2.17046\n",
      "Epoch 80: batch loss = 2.16973\n",
      "Epoch 81: batch loss = 2.16944\n",
      "Epoch 82: batch loss = 2.16905\n",
      "Epoch 83: batch loss = 2.16947\n",
      "Epoch 84: batch loss = 2.17060\n",
      "Epoch 85: batch loss = 2.16854\n",
      "Epoch 86: batch loss = 2.16923\n",
      "Epoch 87: batch loss = 2.16916\n",
      "Epoch 88: batch loss = 2.17023\n",
      "Epoch 89: batch loss = 2.16908\n",
      "Epoch 90: batch loss = 2.17064\n",
      "Epoch 91: batch loss = 2.16970\n",
      "Epoch 92: batch loss = 2.16910\n",
      "Epoch 93: batch loss = 2.16988\n",
      "Epoch 94: batch loss = 2.17029\n",
      "Epoch 95: batch loss = 2.16905\n",
      "Epoch 96: batch loss = 2.16823\n",
      "Epoch 97: batch loss = 2.16955\n",
      "Epoch 98: batch loss = 2.16910\n",
      "Epoch 99: batch loss = 2.16908\n",
      "ðŸŽ¯ GPU Total Time: 54.977 seconds\n",
      "ðŸ“ˆ Average per epoch: 0.550 seconds\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸš€ CPU Benchmark\n",
      "----------------------------------------\n",
      "Epoch 0: batch loss = 2.68269\n",
      "Epoch 1: batch loss = 2.24882\n",
      "Epoch 2: batch loss = 2.21963\n",
      "Epoch 3: batch loss = 2.20849\n",
      "Epoch 4: batch loss = 2.20083\n",
      "Epoch 5: batch loss = 2.19565\n",
      "Epoch 6: batch loss = 2.19396\n",
      "Epoch 7: batch loss = 2.19113\n",
      "Epoch 8: batch loss = 2.18857\n",
      "Epoch 9: batch loss = 2.18611\n",
      "Epoch 10: batch loss = 2.18386\n",
      "Epoch 11: batch loss = 2.18539\n",
      "Epoch 12: batch loss = 2.18202\n",
      "Epoch 13: batch loss = 2.18210\n",
      "Epoch 14: batch loss = 2.18206\n",
      "Epoch 15: batch loss = 2.18092\n",
      "Epoch 16: batch loss = 2.17915\n",
      "Epoch 17: batch loss = 2.17891\n",
      "Epoch 18: batch loss = 2.17737\n",
      "Epoch 19: batch loss = 2.17770\n",
      "Epoch 20: batch loss = 2.17741\n",
      "Epoch 21: batch loss = 2.17760\n",
      "Epoch 22: batch loss = 2.17680\n",
      "Epoch 23: batch loss = 2.17792\n",
      "Epoch 24: batch loss = 2.17456\n",
      "Epoch 25: batch loss = 2.17538\n",
      "Epoch 26: batch loss = 2.17582\n",
      "Epoch 27: batch loss = 2.17508\n",
      "Epoch 28: batch loss = 2.17548\n",
      "Epoch 29: batch loss = 2.17357\n",
      "Epoch 30: batch loss = 2.17452\n",
      "Epoch 31: batch loss = 2.17455\n",
      "Epoch 32: batch loss = 2.17374\n",
      "Epoch 33: batch loss = 2.17394\n",
      "Epoch 34: batch loss = 2.17452\n",
      "Epoch 35: batch loss = 2.17335\n",
      "Epoch 36: batch loss = 2.17172\n",
      "Epoch 37: batch loss = 2.17316\n",
      "Epoch 38: batch loss = 2.17389\n",
      "Epoch 39: batch loss = 2.17232\n",
      "Epoch 40: batch loss = 2.17234\n",
      "Epoch 41: batch loss = 2.17350\n",
      "Epoch 42: batch loss = 2.17154\n",
      "Epoch 43: batch loss = 2.17237\n",
      "Epoch 44: batch loss = 2.17164\n",
      "Epoch 45: batch loss = 2.17218\n",
      "Epoch 46: batch loss = 2.17161\n",
      "Epoch 47: batch loss = 2.17308\n",
      "Epoch 48: batch loss = 2.17103\n",
      "Epoch 49: batch loss = 2.17073\n",
      "Epoch 50: batch loss = 2.16961\n",
      "Epoch 51: batch loss = 2.17196\n",
      "Epoch 52: batch loss = 2.16988\n",
      "Epoch 53: batch loss = 2.17083\n",
      "Epoch 54: batch loss = 2.17200\n",
      "Epoch 55: batch loss = 2.17145\n",
      "Epoch 56: batch loss = 2.17185\n",
      "Epoch 57: batch loss = 2.17082\n",
      "Epoch 58: batch loss = 2.16986\n",
      "Epoch 59: batch loss = 2.17116\n",
      "Epoch 60: batch loss = 2.17005\n",
      "Epoch 61: batch loss = 2.16947\n",
      "Epoch 62: batch loss = 2.17123\n",
      "Epoch 63: batch loss = 2.17111\n",
      "Epoch 64: batch loss = 2.16772\n",
      "Epoch 65: batch loss = 2.16965\n",
      "Epoch 66: batch loss = 2.17079\n",
      "Epoch 67: batch loss = 2.17062\n",
      "Epoch 68: batch loss = 2.17057\n",
      "Epoch 69: batch loss = 2.17032\n",
      "Epoch 70: batch loss = 2.17122\n",
      "Epoch 71: batch loss = 2.16952\n",
      "Epoch 72: batch loss = 2.17031\n",
      "Epoch 73: batch loss = 2.16976\n",
      "Epoch 74: batch loss = 2.17021\n",
      "Epoch 75: batch loss = 2.17038\n",
      "Epoch 76: batch loss = 2.16987\n",
      "Epoch 77: batch loss = 2.17174\n",
      "Epoch 78: batch loss = 2.17131\n",
      "Epoch 79: batch loss = 2.16950\n",
      "Epoch 80: batch loss = 2.16924\n",
      "Epoch 81: batch loss = 2.16937\n",
      "Epoch 82: batch loss = 2.16717\n",
      "Epoch 83: batch loss = 2.16829\n",
      "Epoch 84: batch loss = 2.17045\n",
      "Epoch 85: batch loss = 2.17095\n",
      "Epoch 86: batch loss = 2.16958\n",
      "Epoch 87: batch loss = 2.17109\n",
      "Epoch 88: batch loss = 2.17047\n",
      "Epoch 89: batch loss = 2.16891\n",
      "Epoch 90: batch loss = 2.17000\n",
      "Epoch 91: batch loss = 2.16940\n",
      "Epoch 92: batch loss = 2.17039\n",
      "Epoch 93: batch loss = 2.16934\n",
      "Epoch 94: batch loss = 2.17048\n",
      "Epoch 95: batch loss = 2.17030\n",
      "Epoch 96: batch loss = 2.17080\n",
      "Epoch 97: batch loss = 2.16958\n",
      "Epoch 98: batch loss = 2.16940\n",
      "Epoch 99: batch loss = 2.16925\n",
      "ðŸŽ¯ CPU Total Time: 56.215 seconds\n",
      "ðŸ“ˆ Average per epoch: 0.562 seconds\n",
      "\n",
      "ðŸ“Š BENCHMARK SUMMARY\n",
      "==================================================\n",
      "GPU Time:     54.977s\n",
      "CPU Time:     56.215s\n",
      "GPU Speedup:  1.02x faster\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting benchmarks...\\n\")\n",
    "\n",
    "# GPU benchmark\n",
    "gpu_time = benchmark_training(create_model, create_optimizer, num_epochs=100, device='gpu')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# CPU benchmark  \n",
    "cpu_time = benchmark_training(create_model, create_optimizer, num_epochs=100, device='cpu')\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"GPU Time:     {gpu_time:.3f}s\")\n",
    "print(f\"CPU Time:     {cpu_time:.3f}s\")\n",
    "print(f\"GPU Speedup:  {cpu_time/gpu_time:.2f}x faster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fix_mlx_graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running, the following deps should be installed:\n",
    "\n",
    "`pip install -e .`\n",
    "\n",
    "`pip install torch torch_geometric torch_scatter dgl pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlx-graphs (MXG) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as mlx_nn\n",
    "import mlx.optimizers as mlx_optim\n",
    "\n",
    "import mlx_graphs.nn as mxg_nn\n",
    "import mlx_graphs.datasets as mxg_datasets\n",
    "import mlx_graphs.loaders as mxg_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class MXG_model(mlx_nn.Module):\n",
    "    def __init__(self, layer, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(MXG_model, self).__init__()\n",
    "\n",
    "        self.conv1 = layer(in_dim, hidden_dim)\n",
    "        self.conv2 = layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = layer(hidden_dim, hidden_dim)\n",
    "        self.linear = mxg_nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = mlx_nn.Dropout(p=dropout)\n",
    "\n",
    "    def __call__(self, edge_index, node_features, batch_indices):\n",
    "        h = mlx_nn.relu(self.conv1(edge_index, node_features))\n",
    "        h = mlx_nn.relu(self.conv2(edge_index, h))\n",
    "        h = self.conv3(edge_index, h)\n",
    "        \n",
    "        h = mxg_nn.global_mean_pool(h, batch_indices)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    return mlx_nn.losses.cross_entropy(y_hat, y, reduction=\"mean\")\n",
    "\n",
    "def forward_fn(model, graph):\n",
    "    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "    labels = graph.graph_labels \n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat\n",
    "\n",
    "def setup_training_mxg(dataset, layer, batch_size, hid_size, compile=True):\n",
    "    loader = mxg_loaders.Dataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = MXG_model(\n",
    "        layer=layer,\n",
    "        in_dim=dataset.num_node_features,\n",
    "        hidden_dim=hid_size,\n",
    "        out_dim=dataset.num_graph_classes,\n",
    "    )\n",
    "    mx.eval(model.parameters())\n",
    "\n",
    "    optimizer = mlx_optim.Adam(learning_rate=0.01)\n",
    "    loss_and_grad_fn = mlx_nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "    state = [model.state, optimizer.state, mx.random.state]\n",
    "\n",
    "    if compile:\n",
    "        @partial(mx.compile, inputs=state, outputs=state)\n",
    "        def step(graph):\n",
    "            (loss, y_hat), grads = loss_and_grad_fn(model=model, graph=graph)\n",
    "            optimizer.update(model, grads)\n",
    "    else:\n",
    "        def step(graph):\n",
    "            (loss, y_hat), grads = loss_and_grad_fn(model=model, graph=graph)\n",
    "            optimizer.update(model, grads)\n",
    "\n",
    "    return loader, step, state\n",
    "\n",
    "def train_mxg(loader, step, state=None, epochs=2):    \n",
    "    for _ in range(epochs):\n",
    "        for graph in loader:\n",
    "            step(graph)\n",
    "            mx.eval(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as torch_nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.datasets as pyg_datasets\n",
    "import torch_geometric.loader as pyg_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyG_model(torch.nn.Module):\n",
    "    def __init__(self, layer, in_dim, hidden_dim, out_dim):\n",
    "        super(PyG_model, self).__init__()\n",
    "        \n",
    "        self.conv1 = layer(in_dim, hidden_dim)\n",
    "        self.conv2 = layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = layer(hidden_dim, hidden_dim)\n",
    "        self.lin = torch_nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_pyg(dataset, layer, batch_size, hid_size, compile=True):\n",
    "    loader = pyg_loaders.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = PyG_model(\n",
    "        layer=layer,\n",
    "        in_dim=dataset.num_node_features,\n",
    "        hidden_dim=hid_size,\n",
    "        out_dim=dataset.num_classes,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch_nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    def step(data):\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if compile:\n",
    "        step = torch.compile(step, dynamic=True)\n",
    "\n",
    "    return loader, step, None\n",
    "\n",
    "def train_pyg(loader, step, state=None, epochs=2):\n",
    "    for _ in range(epochs):\n",
    "        for data in loader:\n",
    "            step(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn.pytorch as dgl_nn\n",
    "import dgl.data as dgl_datasets\n",
    "import dgl.dataloading as dgl_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGL_model(torch_nn.Module):\n",
    "    def __init__(self, layer, in_dim, hidden_dim, out_dim):\n",
    "        super(DGL_model, self).__init__()\n",
    "\n",
    "        if \"GATConv\" in str(layer):\n",
    "            self.conv1 = layer(in_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "            self.conv2 = layer(hidden_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "            self.conv3 = layer(hidden_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "        else:\n",
    "            self.conv1 = layer(in_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "            self.conv2 = layer(hidden_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "            self.conv3 = layer(hidden_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "\n",
    "        self.classify = torch_nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        h = F.relu(self.conv3(g, h))\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "            return self.classify(hg.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_dgl(dataset, layer, batch_size, hid_size, compile=True):\n",
    "    loader = dgl_loaders.GraphDataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    model = DGL_model(\n",
    "        layer=layer,\n",
    "        in_dim=dataset[0][0].ndata[\"x\"].shape[1],\n",
    "        hidden_dim=hid_size,\n",
    "        out_dim=dataset.num_classes,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch_nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    def step(data, labels):\n",
    "        out = model(data, data.ndata['x'])\n",
    "        loss = criterion(out, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if compile:\n",
    "        step = torch.compile(step, dynamic=True)\n",
    "    \n",
    "    return loader, step, None\n",
    "\n",
    "def train_dgl(loader, step, state=None, epochs=2):\n",
    "    for _ in range(epochs):\n",
    "        for data, labels in loader:\n",
    "            step(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from itertools import product\n",
    "\n",
    "framework_to_setup = {\n",
    "    \"mxg\": setup_training_mxg,\n",
    "    \"pyg\": setup_training_pyg,\n",
    "    \"dgl\": setup_training_dgl,\n",
    "}\n",
    "\n",
    "framework_to_train = {\n",
    "    \"mxg\": train_mxg,\n",
    "    \"pyg\": train_pyg,\n",
    "    \"dgl\": train_dgl,\n",
    "}\n",
    "def dgl_dataset(name):\n",
    "    pyg_dataset = pyg_datasets.TUDataset(f\".mlx_graphs_data/{name}\", name)\n",
    "    dgl_dataset = dgl_datasets.TUDataset(dataset_name)\n",
    "\n",
    "    for i, (pyg, dgl) in enumerate(zip(pyg_dataset, dgl_dataset.graph_lists)):\n",
    "        dgl_dataset.graph_lists[i].ndata[\"x\"] = pyg.x\n",
    "    \n",
    "    return dgl_dataset\n",
    "\n",
    "framework_to_datasets = {\n",
    "    \"mxg\": lambda name: mxg_datasets.TUDataset(name),\n",
    "    \"pyg\": lambda name: pyg_datasets.TUDataset(f\".mlx_graphs_data/{name}\", name),\n",
    "    \"dgl\": lambda name: dgl_dataset(name)\n",
    "}\n",
    "layer_classes = {\n",
    "    \"mxg\": {\n",
    "        \"GCNConv\": mxg_nn.GCNConv,\n",
    "        \"GATConv\": mxg_nn.GATConv,\n",
    "    },\n",
    "    \"pyg\": {\n",
    "        \"GCNConv\": pyg_nn.GCNConv,\n",
    "        \"GATConv\": pyg_nn.GATConv,\n",
    "    },\n",
    "    \"dgl\": {\n",
    "        \"GCNConv\": dgl_nn.GraphConv,\n",
    "        \"GATConv\": dgl_nn.GATConv,\n",
    "    }\n",
    "}\n",
    "\n",
    "frameworks = [\"dgl\", \"pyg\", \"mxg\"]\n",
    "datasets = [\n",
    "    \"DD\",\n",
    "    # \"BZR_MD\", \n",
    "    # \"MUTAG\",\n",
    "]\n",
    "layers = [\"GCNConv\", \"GATConv\"]\n",
    "\n",
    "batch_size = 64\n",
    "hid_size = 128\n",
    "\n",
    "TIMEIT_REPEAT = 5\n",
    "TIMEIT_NUMBER = 1\n",
    "COMPILE = False\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mx.random.seed(42)\n",
    "dgl.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.set_default_device(mx.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DD\n",
      "==========\n",
      "dgl | GCNConv | 1.623s\n",
      "dgl | GATConv | 3.503s\n",
      "\n",
      "pyg | GCNConv | 2.951s\n",
      "pyg | GATConv | 3.950s\n",
      "\n",
      "mxg | GCNConv | 1.027s\n",
      "mxg | GATConv | 1.365s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark(framework, loader, step, state):\n",
    "    train_fn = framework_to_train[framework]\n",
    "    train_fn(loader, step, state)\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(dataset_name)\n",
    "    print(\"=\" * 10)\n",
    "\n",
    "    for framework in frameworks:\n",
    "        dataset = framework_to_datasets[framework](dataset_name)\n",
    "\n",
    "        for i, layer_name in enumerate(layers):\n",
    "            layer = layer_classes[framework][layer_name]\n",
    "            loader, step, state = framework_to_setup[framework](dataset, layer, batch_size, hid_size, compile=COMPILE)\n",
    "            \n",
    "            times = timeit.Timer(\n",
    "                lambda: benchmark(framework, loader, step, state)\n",
    "            ).repeat(repeat=TIMEIT_REPEAT, number=TIMEIT_NUMBER)\n",
    "\n",
    "            time = min(times) / TIMEIT_NUMBER\n",
    "\n",
    "            print(\n",
    "                \" | \".join(\n",
    "                    [\n",
    "                        f\"{framework}\",\n",
    "                        f\"{layer_name}\",\n",
    "                        f\"{time:.3f}s\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        print(\"\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

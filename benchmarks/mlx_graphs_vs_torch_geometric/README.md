# Benchmarking mlx-graphs vs torch-geometric

Benchmarks are generated by measuring the runtime of some `mlx-graphs` layers, along with their equivalent in [torch_geometric](https://github.com/pyg-team/pytorch_geometric) (PyG) with `mps` and `cpu` backends. For each layer, we measure the runtime of multiple experiments. We propose 2 benchmarks based on these experiments:

* [Detailed benchmark](results/average_benchmark.md): provides the runtime of each experiment.
* [Average runtime benchmark](results/detailed_benchmark.md): computes the mean of experiments. Easier to navigate, with fewer details.

A comprehensive benchmark of core `mlx` operations vs `torch` operations is also available [here]((https://github.com/TristanBilot/mlx-benchmark)).


### Run the benchmark locally

```python
python launcher.py
```
> Note: check that you have the latest dev version of `mlx` installed
